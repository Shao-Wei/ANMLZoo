# Automatize - Random Forest Automata Generator

The Random Forest automata generator, **bin/automatize.py**, is a Python script that generates an ANML file compatible with the Micron Automata Processor(AP) as well as <a href="https://github.com/ted-xie/REAPR">REAPR</a> for Xilinx FPGAs. This ANML file contains the automata representation of the Random Forest algorithm as introduced in:

Tracy II, Tommy & Fu, Al, et al. "Towards machine learning on the Automata Processor." International Conference on High Performance Computing. Springer International Publishing, 2016.

For program dependancies see <a href="https://github.com/tjt7a/ANMLZoo/blob/master/RandomForest/README.md">RandomForest README</a>.

## Inputs

The **bin/automatize.py** script creates an ANML file when given a model pickle file generated by **trainEnsemble.py** (see below).

The command line parameters are as follows:  
`automatize.py <model pickle file> [OPTIONS]`

## Parameter Descriptions

- **`model pickle file`**: This file contains the trained model, serialized as a pickle file, to be transformed into an automata representation.


### [OPTIONS]
Optional parameters:

- **`-a <name of output ANML file>`**: Name of output ANML file (default: model.anml)
- **`--short`**: Make an input file with the first 100 inputs for testing (default: false)
- **`--longer`**: Make a 1000x larger input file to the AP (default: false)
- **`--unrolled`**: Skip compressing the chains into loops. This generates one STE per feature per chains. (This will create a very big output file.) (default: false)
- **`-p`**: Generate a plot of the threshold count distribution of the features (default: false)
- **`-v`**: Print verbose descriptions of each step in program's progress. (default: false)
- **`--circuit`**: Generate circuit-compatible chains and output files (default: false) **EXPERIMENTAL**
- **`--gpu`**: Generate GPU-compatible chains and output files (default: false) **EXPERIMENTAL**


## Example
```
$ bin/automatize.py model.pickle -a mymodel.anml --short -v
```

## Outputs
The **automatize.py** script creates the following files:  
- **model.anml**: This is the ANML-formatted automata file
- **input_file.bin**: A transformed input file for testing (in this case short). It was generated from the testing_data.pickle file.
- **feature_table.pickle**: A serialized feature table that can be used to generate new input data using **generate_test_data.py**.

The resulting Levenshtein automata ANML file can be viewed in Dan Kramp's <a href="https://github.com/dankramp/AutomataLab">AutomataLab</a> viewer.

---

# Simulate Random Forest Automata

Once the automata (**.anml**) and input file (**input_file.bin**) are generated by **automatize.py**, use <a href="https://github.com/jackwadden/VASim">VASim</a> to simulate the automata on the generated test input. VASim simulates spatial automata and produces an output report showing the final classification from each of the trees in the Random Forest. The majority vote of these results than serves as the Forest's net classification.

To run VASim, use the following command line parameters:  
`vasim <RF ANML file> <input file> --report`

## Example
```
$ vasim mymodel.anml input_file.bin --report
```
This will process the input file on the automata contained in the ANML file  and generate a report .txt file with the results.

<p align="center">
<img src="https://raw.githubusercontent.com/jeffudall/ANMLZooCopy/master/RandomForest/images/VASim_output_report.png" width="912" height="639" alt="VASim example">  
</p>

Open the <a href="https://github.com/jeffudall/ANMLZooCopy/blob/master/RandomForest/code/output/reports_0tid_0packet.txt">txt file</a> that is generated to see the results for each input in the training dataset.

Each line of the report file represents a report generated by an automaton in the simulated **anml** file. There are three columns delimited by a **:**. The first column is an input index; this is the index of the input file where the report was generated. The second column is the ID of the state in the automata that reported at that index. Finally, the third column is the Report Code. In the case of the Random Forest automata, this indicates the classification (+1) assigned by the reporting automaton. The report code was set to 1+ the classification value for legacy reasons.

| Report Code | Classification |
|:----------:|:-------------:|
| 1 |  0|
| 2 |  1|
| 3 |  2 |
| 4 |  3|
| 5 |  4|
| 6 |  5 |
| 7 |  6|
| 8 |  7|
| 9 |  8 |
| 10 |  9 |

For this example, there are 10 decision trees in the ensemble, each of which must report once per reporting cycle. Therefore, we expect 10 unique reports per reporting cycle.

In the example below, a Random Forest was trained on MNIST canned data (see **Input File Generator - trainEnsemble.py** below). All reports from each tree report on the same cycle (as show). As an example, byte 7259 of the input file generated 10 unique reports, one per tree. All trees reported the same report code, 1, which indicates that the input that ends in byte 7259 is classified by the Random Forest as a 0 digit. The sample ending in byte 7686 was not unanimously classified, with the majority of trees recognizing the input as a 5. For the input ending in byte 8113, it was almost unanimously recognized as option 4 (three) but one tree classified it with report code 6, or the 5 digit.

<p align="center">
<img src="https://raw.githubusercontent.com/jeffudall/ANMLZooCopy/master/RandomForest/images/VASim_results.png" width="250" height="511" alt="VASim output">  
</p>

---

# Train Ensemble

## Inputs
**trainEnsemble.py** trains a decision tree ensemble model when given a canned dataset, *c*, OR training and testing data (*t*, *x*), a model type, *m*, a depth of decision trees, *d* OR number of leaf nodes per tree, *l*, and a number of trees, *n* are provided as input.

The command line parameters are as follows:

`trainEnsemble.py -c <canned dataset> -m <model type> -d <depth of decision trees> -n <number of trees in ensemble>`

## Usage Parameter Descriptions

### -c canned_dataset
This parameter specifies the name of the canned dataset used by **trainEnsemble.py** from SKLEARN.

MNIST, a canned dataset included in SciKit LEARN, is provided as an example.

### -m model_type
This parameter specifies model type
- **`rf`** = **Random Forest**
- **`brt`** = **Boosted Regression Trees**
- **`ada`** = **Adaboost Classifier**

### -d depth
This parameter specifies the maximum depth of the decision tree learners.

### -n number of trees
This parameter specifies the number of decision trees in the ensemble.

### [OPTIONS]
You can also specifiy these optional parameters:
- **`-l <number of leaves>`**: Instead of depth you can specify number of leaves for decision trees.  
- **`-j <number of jobs>`**: You can specify the number of jobs to run in parallel for fit/predict.
- **`-t <training npz file name>`**: Name of training data .npz file.   
- **`-x <testing data npz file name>`**: Name of testing data .npz file.
- **`-f <number of features>`**: The number of features to use for training.
- **`--report`**: You can specify the name of the report file that contains infromation about the trained model. (default is *"report.txt"*)
- **`--metric`**: Choose the metric used for evaluation.  
    - **`acc`**: Accuracy score (default)  
    - `f1` : F1 score   
    - `mse`: Mean Squared Error  
- **`--feature_importance`**: Dump the feature importance values of the trained ensemble.
- **`-p <Name of predictions file>`**: Name of the file containing model predictions for testing (default: predictions.txt)
- **`-v`**: Print verbose descriptions of each step in program's progress (default: false)


## Example
```
$ trainEnsemble.py -c mnist -m rf -d 8 -n 10
```
This will train a Random Forest using the MNIST canned dataset with 10 trees in the ensemble, each with a maximum depth of 8.

<p align="center">
<img src="https://raw.githubusercontent.com/jeffudall/ANMLZooCopy/master/RandomForest/images/test_run_train.png" width="700" height="167" alt="trainEnsemble screen shot">  
</p>

## Outputs
The **trainEnsemble.py** script creates the following files:  
- **model.pickle**: a serialized Scikit Learn decision tree ensemble model  
- **report.txt**: a file that contains the parameters used for training the model  
- **testing_data.pickle**: a serialized file containing the testing data
- **training_data.pickle**: a serialized file containing the training data

---

# Majority Voter - bin/classify.py

## Inputs
The Random Forest classify script, **classify.py**, reads a reports file generated by VASIM, and generates a file containing the resulting classifications, one per input.

The command line parameters are as follows:

`classify.py reports_0tid_0packet.txt`

## Usage Parameter Descriptions

### reports file
This file is generated by VASIM with the **-r** flag (or **--report**), and contains one line per report.


### [OPTIONS]
Optional parameters:
- **`-o <classification output filename>`**: Optionally specify the classification output filename. (default: classifications.txt)


## Outputs
The resulting output file contains one line per classification in the following format:

- **input index : classification**

---

# Optional - Test the CPU throughput of the model
In order to get an approximation of the performance of the same Random Forest on a standard CPU processor, use the **bin/test_cpu.py** script to calculate the average throughput of the model on the CPU. Run **test_cpu.py** in the same directory as your generated model files.

## Usage Parameters
All parameters are optional parameters:

- **`-n <test iterations>`**: Number of test iterations (defaults to 1000)
- **`-j <threads>`**: Number of threads used to run the model
- **`-m <model file>`**: The serialized model file name (defaults to *model.pickle*)
- **`-t <testing data>`**: The testing data output file name (defaults to *testing_data.pickle*)
- **`-v`**: Print verbose descriptions of each step in program's progress.

## Example
When using the default settings:
```
$ test_cpu.py
```
<p align="center">
<img src="https://raw.githubusercontent.com/jeffudall/ANMLZooCopy/master/RandomForest/images/test_cpu_run_1.png" width="631" height="90" alt="test_cpu screen shot">  
</p>

If using custom settings:
```
$ test_cpu.py -m <model file> -t <testing data> -n <test iterations> -j <threads>
```

## Output
The resulting throughput is a measurement in kilo-samples classified per wall-clock second.

---

# Optional - Generate Custom Test Input
If additional test input or runtime data is to be used with the Random Forest automata, it is necessary to translate the input data into range symbols for processing. **bin/generate_test_data.py** allows the user to transform feature values into label values. It does this, by using the **feature_table.pickle** file generated by **automatize.py**.

## Inputs
The **generate_test_data.py** script will generate an input stream for the Random Forest automata when provided a **feature_table.pickle** and an input pickle file.

The command line parameters are as follows:

`generate_test_data.py [OPTIONS] <testing data.pickle> `


## Optional Parameters

- **`-o output_file`**: Provide the name of the generated output file (default: input)
- **`--short`**: Generate a short version of the input data (100 samples)
- **`-v`**: Print verbose descriptions of each step in program's progress.

## Example
When using the default settings:
```
$ generate_test_data.py
```

## Output
This script generates an output file called **input_file**, that can be processed by the automata.

---

# Citing This Code

If using this code for research purposes, please cite the below paper which introduces the contained algorithms.
Citation:

Tracy II, Tommy & Fu, Al, et al. "Towards machine learning on the Automata Processor." International Conference on High Performance Computing. Springer International Publishing, 2016.
